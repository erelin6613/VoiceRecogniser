{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#!pip install ../input/transformers/transformers-master/\n#!pip install keras_preprocessing\n#!pip install bert","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport os\nimport librosa\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow.keras.backend as K\n#from sklearn.metrics.pairwise import cosine_similarity\nfrom torchaudio import load\nfrom keras_preprocessing.sequence import pad_sequences\nfrom transformers import BertModel, TFBertModel, BertTokenizer\nimport numpy as np\nfrom math import sqrt\nfrom tqdm import notebook","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dir = '../input/common-voice'\naudio_dir = os.path.join(data_dir, 'cv-valid-train')\n#/cv-valid-train'\nsound_len = 10\nbert_path = '../input/bert-base-uncased'\nbert_vocab_path = 'vocab.txt'\nbert_model_path = 'bert-base-uncased/pytorch_model.bin'\nmax_len_text = 30\nmax_len_fft = 300000\nwindow = 40\nbatch_size = 1\nsess = tf.compat.v1.Session()\nbert_path = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\nepochs = 2","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class VoiceInstance:\n    \n    def __init__(self, file, tokenizer, model=None, sound_len=10**10, text=None):\n        \n        self.file = file\n        self.text = text\n        self.tokenizer = tokenizer\n        self.model = model\n        self.mfcc = self._transform_audio()\n        self._get_embeddings()\n        \n    @staticmethod\n    def pad_seq_text(sequence):\n        return np.pad(sequence, (0, max_len_text-len(sequence)), \n                      mode='constant')\n    \n    @staticmethod\n    def pad_seq_fft(sequence):\n        return np.pad(sequence, (0, max_len_fft-len(sequence)), \n                      mode='constant')\n    \n    def _transform_audio(self):\n        \n        waveform, rate = load(file)\n        new_rate = rate/100\n        self.fft, self.frequency = self._get_fft(waveform, new_rate)\n        #self.mfcc = self._get_mfcc(waveform, new_rate)\n        \n    def _get_mfcc(self, waveform, sample_rate=22000):\n        \n        mfcc_tensor = tf.signal.dct(waveform.numpy())\n        return mfcc_tensor\n    \n    def _get_fft(self, waveform, rate):\n        \n        length = len(waveform)\n        frequency = np.fft.rfftfreq(length)\n        fft = np.fft.rfft(waveform)/length\n        fft = np.array([sqrt(np.real(x)**2 + np.imag(x)**2) for x in fft[0]])\n        return self.pad_seq_fft(fft), frequency\n    \n    def _get_embeddings(self):\n        \n        def tokenize_text(text):\n            return self.tokenizer.tokenize(text)\n        \n        def get_ids(tokens):\n            return self.tokenizer.convert_tokens_to_ids(tokens)\n        \n        if self.text:\n            self.tokens = tokenize_text(self.text)\n            emb = np.array(get_ids(self.tokens))\n            emb = self.pad_seq_text(emb)\n            self.embeddings = emb\n            #self.embeddings = self.model.call(emb.reshape(1, -1))\n","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_voice_model(max_len_fft=max_len_fft, batch_size=1):\n    \n    input_layer = tf.keras.layers.Input(shape=(max_len_fft, ), batch_size=batch_size)\n    norm_1 = tf.keras.layers.BatchNormalization()(input_layer)\n    reshaped = tf.keras.layers.Reshape((-1, max_len_fft))(norm_1)\n    lstm_1 = tf.keras.layers.LSTM(256, return_sequences=True)(reshaped)\n    flatten = tf.keras.layers.Flatten()(lstm_1)\n    dense_1 = tf.keras.layers.Dense(256, activation='relu')(flatten)\n    \n    model = tf.keras.Model(inputs=[input_layer], outputs=[dense_1])\n    model.compile(optimizer='Adam', loss='mean_squared_error', metrics=['accuracy'])\n    return model\n\"\"\"\nclass VoiceModel(tf.keras.Model):\n    \n    def __init__(self, max_text_len=max_len_text,\n                batch_size=batch_size,\n                max_len_fft=max_len_fft, embs=768):\n        super(VoiceModel, self).__init__(self)\n        #self.input_layer = tf.keras.layers.Input(shape=(max_len_fft, ), batch_size=batch_size)\n        self.lstm_1 = tf.keras.layers.LSTM(256, return_sequences=True)\n        self.dropout = tf.keras.layers.Dropout(0.3)\n        self.lstm_2 = tf.keras.layers.LSTM(256)\n        self.flatten = tf.keras.layers.Flatten()\n        self.dense = tf.keras.layers.Dense(256, activation='relu')\n        self.output_layer = tf.keras.layers.Dense(embs, activation='tanh')\n        \n    def call(self, x):\n        \n        #x = self.input_layer(x)\n        x = self.lstm_1(x)\n        x = self.dropout(x)\n        x = self.lstm_2(x)\n        x = self.flatten(x)\n        x = self.dense(x)\n        \n        return self.output_layer(x)\n\"\"\"","execution_count":70,"outputs":[{"output_type":"execute_result","execution_count":70,"data":{"text/plain":"\"\\nclass VoiceModel(tf.keras.Model):\\n    \\n    def __init__(self, max_text_len=max_len_text,\\n                batch_size=batch_size,\\n                max_len_fft=max_len_fft, embs=768):\\n        super(VoiceModel, self).__init__(self)\\n        #self.input_layer = tf.keras.layers.Input(shape=(max_len_fft, ), batch_size=batch_size)\\n        self.lstm_1 = tf.keras.layers.LSTM(256, return_sequences=True)\\n        self.dropout = tf.keras.layers.Dropout(0.3)\\n        self.lstm_2 = tf.keras.layers.LSTM(256)\\n        self.flatten = tf.keras.layers.Flatten()\\n        self.dense = tf.keras.layers.Dense(256, activation='relu')\\n        self.output_layer = tf.keras.layers.Dense(embs, activation='tanh')\\n        \\n    def call(self, x):\\n        \\n        #x = self.input_layer(x)\\n        x = self.lstm_1(x)\\n        x = self.dropout(x)\\n        x = self.lstm_2(x)\\n        x = self.flatten(x)\\n        x = self.dense(x)\\n        \\n        return self.output_layer(x)\\n\""},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_loss = tf.keras.metrics.KLDivergence(name='train_loss')\ntrain_accuracy = tf.keras.metrics.CosineSimilarity(name='train_accuracy')\ntest_loss = tf.keras.metrics.KLDivergence(name='test_loss')\ntest_accuracy = tf.keras.metrics.CosineSimilarity(name='test_accuracy')\nloss = tf.keras.losses.KLDivergence()\noptimizer = tf.keras.optimizers.Adam()\n\n#instance = \n#model = VoiceModel(instance.fft, instance.embeddings)\nmodel = build_voice_model()","execution_count":71,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'Model' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-71-67cf9b51f9f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#instance =\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#model = VoiceModel(instance.fft, instance.embeddings)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_voice_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-70-ded82ceca5be>\u001b[0m in \u001b[0;36mbuild_voice_model\u001b[0;34m(max_len_fft, batch_size)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdense_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_layer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdense_1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean_squared_error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'Model' is not defined"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"@tf.function\ndef train_step(images, labels):\n    predictions = model(images, training=True)\n    loss = loss_object(labels, predictions)\n    gradients = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n    train_loss(loss)\n    train_accuracy(labels, predictions)\n","execution_count":57,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@tf.function\ndef test_step(images, labels):\n    predictions = model(images, training=False)\n    t_loss = loss(labels, predictions)\n    test_loss(t_loss)\n    test_accuracy(labels, predictions)\n","execution_count":58,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n#model = TFBertModel.from_pretrained('bert-base-cased')\ninfo_frame = pd.read_csv('../input/common-voice/cv-valid-train.csv')\n\nprepocessed_frame = pd.DataFrame(columns=['filename', 'tokens', 'fft', 'hidden_emb'])\n\nfor epoch in range(epochs):\n\n    for i in notebook.tqdm(info_frame.index):\n        if info_frame.loc[i, 'down_votes'] > 2:\n            continue\n        train_loss.reset_states()\n        train_accuracy.reset_states()\n        test_loss.reset_states()\n        test_accuracy.reset_states()\n\n        file = os.path.join(audio_dir, info_frame.loc[i, 'filename'])\n        instance = VoiceInstance(file=file, \n                                 tokenizer=tokenizer,\n                                 text=info_frame.loc[i, 'text'])\n        #print(tf.reshape(instance.embeddings[0], -1, 1).shape)\n        #print(instance.__dir__())\n        print(instance.fft, instance.embeddings)\n        model.fit(instance.fft, instance.embeddings)\n        #print(instance.embeddings[1].shape)\n        prepocessed_frame = prepocessed_frame.append({'index': i,\n                                                     'tokens': instance.tokens,\n                                                     'fft': instance.fft,\n                                                    'hidden_emb': instance.embeddings[1]}, \n                                                     ignore_index=True)\n    print(prepocessed_frame.head())\n    prepocessed_frame.to_csv('prepocessed_frame.csv')\n    ","execution_count":59,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=195776.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1043f148cd4a406b8bdbb5e8cbdf4014"}},"metadata":{}},{"output_type":"stream","text":"[ 8.24768387 10.0985757   9.56836605 ...  0.          0.\n  0.        ] [3858 1106 6239  184 2354 1116 1105 2812 1172 1103 1385 2226 1125 1163\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0]\n\n","name":"stdout"},{"output_type":"error","ename":"ValueError","evalue":"in converted code:\n\n    <ipython-input-55-6490e3ee3e36>:18 call  *\n        x = self.lstm_1(x)\n    /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/recurrent.py:644 __call__\n        return super(RNN, self).__call__(inputs, **kwargs)\n    /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py:737 __call__\n        self.name)\n    /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/input_spec.py:177 assert_input_compatibility\n        str(x.shape.as_list()))\n\n    ValueError: Input 0 of layer lstm_12 is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: [None, 1]\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-59-5b79bdb33031>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m#print(instance.__dir__())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;31m#print(instance.embeddings[1].shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         prepocessed_frame = prepocessed_frame.append({'index': i,\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    594\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    644\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m     x, y, sample_weights = standardize(\n\u001b[0;32m--> 646\u001b[0;31m         x, y, sample_weight=sample_weights)\n\u001b[0m\u001b[1;32m    647\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0madapter_cls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mListsOfScalarsDataAdapter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandardize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2344\u001b[0m     \u001b[0;31m# First, we build the model on the fly if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2345\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2346\u001b[0;31m       \u001b[0mall_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_model_with_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2347\u001b[0m       \u001b[0mis_build_called\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2348\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_build_model_with_inputs\u001b[0;34m(self, inputs, targets)\u001b[0m\n\u001b[1;32m   2570\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2571\u001b[0m       \u001b[0mcast_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2572\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2573\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mprocessed_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_dict_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_set_inputs\u001b[0;34m(self, inputs, outputs, training)\u001b[0m\n\u001b[1;32m   2657\u001b[0m           \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2659\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2660\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2661\u001b[0m         \u001b[0;31m# This Model or a submodel is dynamic and hasn't overridden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    771\u001b[0m                     not base_layer_utils.is_in_eager_or_tf_function()):\n\u001b[1;32m    772\u001b[0m                   \u001b[0;32mwith\u001b[0m \u001b[0mauto_control_deps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAutomaticControlDependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0macd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 773\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    774\u001b[0m                     \u001b[0;31m# Wrap Tensors in `outputs` in `tf.identity` to avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m                     \u001b[0;31m# circular dependencies.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: in converted code:\n\n    <ipython-input-55-6490e3ee3e36>:18 call  *\n        x = self.lstm_1(x)\n    /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/recurrent.py:644 __call__\n        return super(RNN, self).__call__(inputs, **kwargs)\n    /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py:737 __call__\n        self.name)\n    /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/input_spec.py:177 assert_input_compatibility\n        str(x.shape.as_list()))\n\n    ValueError: Input 0 of layer lstm_12 is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: [None, 1]\n"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}